\documentclass[../../main.tex]{subfiles}
\begin{document}
\section{Convergence of eigenvalues in Finite Element Analysis}
In this section, the work in the textbook \cite[p.228-236]{SF73} is discussed.\\

Let $H$ be a Hilbert space with the inner product $(\cdot,\cdot)$ and induced norm $||\cdot||$. Let $V$ be a linear subspace of $H$, with inner product defined by the bilinear form $b(\cdot,\cdot)$. It is assumed that this bilinear form $b$ is symmetric.

\subsection{General eigenvalue problem}
Find a function $u \in H$ and number $\lambda \in \mathcal{R}$ such that
\begin{equation}
	b(u,v) = \lambda (u,v) \label{GEVP}
\end{equation} for some $v \in V$.

\subsection{Preparatory theory and notation}
Some theory is required that is presented in \cite{SF73}.\\

\textbf{Rayleigh quotient}
\begin{equation*}
	R(v) = \frac{b(v,v)}{(v,v)} \ \text { for } v \in V.
\end{equation*}

\textbf{minmax Principle}\\
Let T denote the class of subspaces of $V$ that jas dimension $j$. Then
\begin{equation*}
	\lambda_j = \min_{s\in T}\max_{v \in S} R(v).
\end{equation*}

\textbf{Rayleigh-Ritz Projection}\\
If $u \in V$, then $Pu$ is its component in the subspace $S^h$.
\begin{equation*}
	b(u-Pu,v^h) = 0 \ \text{ for all } v \in S^h.
\end{equation*}

Consider a interpolation operator $\Pi$. This projection is linear, i.e.
\begin{align*}
	\Pi(f + g) & = \Pi f + \Pi g,\\
	\Pi(\alpha f) & = \alpha \Pi f \ \text{ for a constant } \alpha. 
\end{align*}

Define the interval $I_e = [a, a+h]$. A necessary condition is for the operator $\Pi$ is that when $\Pi u$ is restricted to the interval $I_e$, this must equal the projection of $u$ restricted to the interval $I_e$. This can be written as
\begin{equation*}
	\left[ \Pi u \right]_{I_{e}} = \Pi_e [u]_{I_{e}}.
\end{equation*}

The following notation is introduced.
\begin{itemize}
	\item[] $\mathcal{P}_j(I_e)$: Is the set of all polynomials on the interval $I_e$ of degree at most $j$.
	\item[] $r(\Pi_e)$: If the range of $\Pi_e$ is contained in $\mathcal{P}_j(I_e)$ and $k<j$ is the largest integer such that $\Pi_e f = f$ for each $f \in \mathcal{P}_j(I_e)$, then $r(\Pi_e) = k$.
	\item[] $s(\Pi_e)$: Is a integer and the largest order derivative used in the definition of $\Pi_e$.
\end{itemize}

From the textbook \cite{ORXX}, the following approximation for finite elements is given verbatim:
\newtheorem*{Interpolation}{Theorem}
\begin{Interpolation}[The Interpolation Theorem for Finite Elements] 
	Let $\Omega$ be an open bounded domain in $\mathcal{R}^n$ satisfying the cone condition. Let $k$ be a fixed integer and $m$ an integer such that $0\leq m \leq k+1$. Let $\Pi \in L(H^{k+1}(\Omega), H^{m}(\Omega))$ be such that
	\begin{eqnarray}
	    \Pi u = u \ \ \ \ \textrm{ for all } u \in \mathcal{P}_k(\Omega)
	\end{eqnarray}
	Then for any $u \in H^{k+1}(\Omega)$ and for sufficiently small $h$, there exists positive a constant $C$, independent of $u$ and $h$, such that
	\begin{eqnarray}
	    ||u - \Pi u||_{H^m(\Omega)} \leq C \frac{h^{k+1}}{p^m} |u|_H^{k+1}(\Omega)
	\end{eqnarray}
	where $|u|_H^{k+1}(\Omega)$ is the seminorm.
\end{Interpolation}

For the requirements of this dissertation, this is too general and can be simplified with assumptions. The first assumption is a reiteration that the eigenvalue problem is one-dimensional. Another assumption is that the basis of $S^h$ consists of polynomials. With these assumptions, the semi-norm $|u|_H$ is equal to the norm $||u||$. This approximation theorem can be rewritten as
\newtheorem{Interpolation_2}{Theorem}
\begin{Interpolation_2}
	Suppose there exists an integer $k$ such that for each element
	\begin{equation*}
		s(\Pi_e) + 1 \leq k \leq r(\Pi_e) +1.
	\end{equation*}
	Then there exists a constant $C$ such that for any $u \in C^k_+(I)$,
	\begin{equation*}
		||(\Pi u)^(m) - u^(m)|| \leq C h^{k-m} || u^(k)|| \ \text{ for } m = 0,1,...,k.
	\end{equation*}
\end{Interpolation_2}

It is well known that the eigenvalues of a eigenvalue problem such as \eqref{GEVP} form an infinite sequence $(\lambda_i)$. Assume that the eigenvalues $\lambda_i$ can be ordered $\lambda_1 \leq \lambda_2 \leq \lambda_3 \leq ...$ and denote the corresponding eigenfunctions $u_i$.\\

Since any multiple of a eigenfunction is still an eigenfunction, the eigenfunctions can be normalized so that $||u_i|| = 1$ for all $i$.\\

Fix $m \in \mathbb{N}$. Let $\left\{ \phi_i \in V | i = 1,2,...,m \right\}$ be a set of linear independent admissible basis functions. Define $S^h := \text{span}\left\{\phi_i \in V | i = 1,2,...,m\right\}$.\\

Since $S^h$ is finite dimensional, the ordering of the eigenvalues still hold. The eigenvalues in the space $S^h$ is denoted as $\lambda_i^h$ with corresponding normalized eigenfunction $u^h_i$ for $i = 1,2,...,m$.\\

The space $S^h$ is a finite dimensional subspace of $V$. Therefore any subspace of $S^h$ will also be a subspace of $V$. So the minmax principle applies and a lower bound for the eigenvalues can be found:
\begin{equation}
	\lambda_i \leq \lambda_i^h.
\end{equation}


\subsection{Estimating the eigenvalues.}
Let $E_j \in V$ denote the eigenspace spanned by the exact eigenvalues $\left\{u_1,u_2,...,u_j \right\}$ for $j = 1,2,...,m$.\\

Then consider the subspace $S_j$ where
\begin{equation*}
	S_j = PE_j \ \text{ for } j = 1,2,...,m.
\end{equation*} The elements $Py_i$ are the projections of the eigenfunctions $u_i$ into the space $S^h$. These projections $Py_i$ are not necessarily equal to $u^h_j \in S^h$, but it can be shown that they are close.\\

Let $B_m = \left\{u \in e_m \ | \ ||u|| = 1 \right\}$ and define $\mu_m = \inf\left\{(Pu,Pu \ | \ u \in B_m)\right\}$.\\

The next steps in the textbook \cite{SF73} contains proofs with multiple results. In an attempt to better understand the results, the proofs are broken up into smaller proofs.\\

The first step to obtain estimates for the eigenvalues, it to show that the elements of $B_m$ are linearly independent.

\newtheorem{Prop_1}{Proposition}
\begin{Prop_1}
 $\mu_{m} > 0$ if and only if $\dim S_{m} = m$.
\end{Prop_1}
\begin{proof}
	To show that the dimension of $S_m = m$, suppose that the elements of $B_m$ are linearly dependent. Then there exists a $u \in B_m$ such that $Pu = 0$ and consequently $\mu_m = 0$. The result follows from the contra-positive.
\end{proof}

\newtheorem{Prop_2}[Prop_1]{Proposition}
\begin{Prop_2}
	$\lambda^{h}_{m} \leq \max R(Pu) \ \text{ for } u \in B_{m}$\\
\end{Prop_2}
\begin{proof}
	Since $\dim S_{m} = m$, following from the minmax principle
	\begin{equation*}
		\lambda_m^h \leq \max R(v) \ \text{ for } v \in S_m.
	\end{equation*}

	Take an arbitrary nonzero $v \in S_m$. Then there exists a $Py \in E_m$ such that $v = Py$.\\
 
	Now we take an arbitrary $v \in S_{m}$, $v \neq 0$. Then there exists a $u \in E_{m}$ such that $Py = v$. This $Py$ is the projection into $S_m$ of some $u \in E_m$ (which is also $\displaystyle \frac{1}{||u||}u \in B_m$).\\
	
	Next a linearity property for $R$ can be proven. 
	\begin{eqnarray*}
	R(\frac{1}{||u||}u) &=& \frac{b\left(\frac{1}{||u||}u,\frac{1}{||u||}u\right)}{\left(\frac{1}{||u||}u,\frac{1}{||u||}u\right)},\\
						&=& \frac{b(u,u)}{(u,u)},\\
						&=& R(u).
	\end{eqnarray*}
	So finally,
	\begin{eqnarray*}
	\lambda_{m}^{h} &\leq & \max R(v) \ \text{ for } \ v \in S_{m},\\
						&=& \max R(Pu) \ \text{ for } \ u \in E_{m},\\
						&=& \max R(Pu) \ \text{ for } \ u \in B_{m}.
	\end{eqnarray*}
\end{proof}

%\newpage
In the textbook, the authors show that the eigenfunctions are orthogonal. But they do so using matrix representations of the eigenvalue problem. A different method can be used to show this.\\

Pick any $i,j \in \mathbb{N}$ such that $i,j \leq m$. Let $v_{i},v_{j} \in V$ be arbitrary, then
\begin{eqnarray*}
	b(u_{i},u_{i}) &=& \lambda_{i}( u_{i},u_{i}),\\
	b(u_{j},u_{j}) &=& \lambda_{j}( u_{j},u_{j}).
\end{eqnarray*}
Now pick $u_{i} = u_{j}$ and $u_{j} = u_{i}$. Then
\begin{eqnarray*}
	b(u_{i},u_{j}) &=& \lambda_{i}( u_{i},u_{j}),\\
	b(u_{j},u_{i}) &=& \lambda_{j}( u_{j},u_{i}).
\end{eqnarray*}
Then using the symmetry of $b(\cdot,\cdot)$ and $( \cdot, \cdot )$, and subtracting,
\begin{eqnarray*}
	0 &=& (\lambda_{i} - \lambda_{j})( u_{i}, u_{j} )
\end{eqnarray*}
So if $i \neq j$ then $\lambda_{i} \neq \lambda_{j}$.\\

Therefore $u_{i}$ and $u_{j}$ must be orthogonal. And as mentioned previously, $||u|| = 1$.


\newtheorem{Lem_1}{Lemma}
\begin{Lem_1}
	$\displaystyle \lambda_{m}^{h} \leq \frac{\lambda_{m}}{\mu_{m}^{h}}$
\end{Lem_1}
\begin{proof}
	Consider the linearity of the bilinear form $b$, and fact that any $u \in E_m$ can be expressed as a linear combination $u = \sum_{i=1}^{m} c_{i}u_{i}$. Then
	\begin{eqnarray*}
	b(u,u) &=& b\left(\sum_{i=1}^{m} c_{i}u_{i},\sum_{j=1}^{m} c_{j}u_{j}\right),\\
			&=& \sum_{i=1}^{m} c_{i}\sum_{j=1}^{m} c_{j} b(u_{i},u_{j}).
	\end{eqnarray*}

	The summation parameters can be merged into a single parameter. Then
	\begin{eqnarray*}
		b(u,u)  & = & \sum_{i=1}^{m} c_{i}^{2} \lambda_{i} u_i,\\
				& \leq & \lambda_{m}\sum_{i=1}^{m} c_{i}^{2} u_i,\\
				& = & \lambda_{m}||u||^2.
	\end{eqnarray*} for all $u \in B_m$.\\
	
	
	$b(u,u) = \sum_{i=1}^{m} c_{i}^{2} \lambda_{i} \leq \lambda_{m}\sum_{i=1}^{m} c_{i}^{2} =\lambda_{m} ||u||^{2}$ for all $u \in E_{m}$.\\
	And since $B_{m} \subset E_{m}$, we have $b(Pu,Pu) \leq \lambda_{m}$ for all $u\in B_{m}$\\
	
	
	So now from the definition of $\mu_{m}^{h}$ and the Rayleigh quotient, we have
	\begin{eqnarray*}
		R(Pu) &=& \frac{b(Pu,Pu)}{(Pu,Pu)}\\
			&=& \frac{b(Pu,Pu)}{||Pu||^{2}}\\
			&\leq & \frac{\lambda_{m}}{\mu_{m}^{h}}
	\end{eqnarray*}
	This together with Proposition 7.1 gives
	\begin{eqnarray*}
		\lambda_{m}^{h} \leq \frac{\lambda_{m}}{\mu_{m}^{h}}
	\end{eqnarray*}
\end{proof}
\begin{comment}
\textbf{Proof:} Let $Y = \sum_{i=1}^{m} c_{i}u_{i}$.
\begin{eqnarray*}
b(u,u) &=& b\left(\sum_{i=1}^{m} c_{i}u_{i},\sum_{j=1}^{m} c_{j}u_{j}\right)\\
		&=& \sum_{i=1}^{m} c_{i}\sum_{j=1}^{m} c_{j} b(u_{i},u_{j})
\end{eqnarray*}
\textbf{RTP:} $\left\{u_{1},u_{2},...,u_{m}\right\}$ is orthonormal.\\
Pick any $i,j \in \mathbb{N}$ such that $i,j \leq m$. Let arbitrary $v_{i},v_{j} \in V$ be arbitrary, then
\begin{eqnarray*}
	b(u_{i},u_{i}) &=& \lambda_{i}( u_{i},u_{i})\\
	b(u_{j},u_{j}) &=& \lambda_{j}( u_{j},u_{j})
\end{eqnarray*}
Now we pick $u_{i} = u_{j}$ and $u_{j} = u_{i}$. Then
\begin{eqnarray*}
	b(u_{i},u_{j}) &=& \lambda_{i}( u_{i},u_{j})\\
	b(u_{j},u_{i}) &=& \lambda_{j}( u_{j},u_{i})
\end{eqnarray*}
Then using the symmetry of $b(\cdot,\cdot)$ and $( \cdot, \cdot )$, and subtracting, we get
\begin{eqnarray*}
 0 &=& (\lambda_{i} - \lambda_{j})( u_{i}, u_{j} )
\end{eqnarray*}
So if $i \neq j$, $\lambda_{i} \neq \lambda_{j}$. Thus we must have that $u_{i}$ and $u_{j}$ are orthogonal. And since by the way $u_{i}$ is defined, $||u_{i}|| = 1$. So thus $\left\{u_{1},u_{2},...,u_{m}\right\}$ is orthonormal.\\


So then $b(u,u) = \sum_{i=1}^{m} c_{i}^{2} \lambda_{i} \leq \lambda_{m}\sum_{i=1}^{m} c_{i}^{2} =\lambda_{m} ||u||^{2}$ for all $u \in E_{m}$.\\
And since $B_{m} \subset E_{m}$, we have $b(Pu,Pu) \leq \lambda_{m}$ for all $u\in B_{m}$\\


So now from the definition of $\mu_{m}^{h}$ and the Rayleigh quotient, we have
\begin{eqnarray*}
	R(Pu) &=& \frac{b(Pu,Pu)}{(Pu,Pu)}\\
		&=& \frac{b(Pu,Pu)}{||Pu||^{2}}\\
		&\leq & \frac{\lambda_{m}}{\mu_{m}^{h}}
\end{eqnarray*}
This together with Proposition 7.1 gives
\begin{eqnarray*}
	\lambda_{m}^{h} \leq \frac{\lambda_{m}}{\mu_{m}^{h}}
\end{eqnarray*}\qed
\end{comment}

Now since $\lambda_{i}^{h} \geq \lambda_{i}$ and by Lemma 7.1, we must then have that $0 < \mu_{m}^{h} \leq 1$. Now define the error estimate as
\begin{eqnarray*}
\sigma_{m}^{h} = 1 - \mu_{m}^{h}
\end{eqnarray*}

%\newpage

\textbf{Corollary 7.1:} $0 \leq \sigma_{m}^{h} < 1$ and $\lambda_{m}^{h} - \lambda_{m} \leq \lambda_{m}^{h}\sigma_{m}^{h}$\\

\begin{comment}
\textbf{Proof:}
\begin{eqnarray*}
\lambda_{m}^{h} & \leq & \frac{\lambda_{m}}{\mu_{m}^{h}} \ \ \textrm { (by Lemma 7.1)} \\
\lambda_{m}^{h}\mu_{m}^{h}& \leq & \lambda_{m} \\
-\lambda_{m}& \leq & -\lambda_{m}^{h}\mu_{m}^{h} \\
\lambda_{m}^{h} -\lambda_{m}& \leq & \lambda_{m}^{h} -\lambda_{m}^{h}\mu_{m}^{h}\\
\lambda_{m}^{h} -\lambda_{m}& \leq & \lambda_{m}^{h}\sigma_{m}^{h}
\end{eqnarray*}\qed


So from this Corollary 7.1, we see that to prove that our approximate eigenvalues converge to the exact eigenvalues. The only thing then left to prove is  that $\sigma_{m}^{h} \rightarrow 0$ as $h \rightarrow 0$.\\
\\


\textbf{Proposition 7.1} $\sigma_{m}^{h} = \max\left\{ 2( u,u-Pu )-||u-Pu||^{2} \ | \ u \in B_{m} \right \}$\\


\textbf{Proof:}  Let $u \in B_{m}$. Then
\begin{eqnarray*}
||u - Pu||^{2} &=& ( u - Pu, u - Pu ) \\
				&=& ( u, u ) - 2 ( u, Pu ) + ( Pu, Pu ) \\
				&=& 2( u, u ) - 2 ( u, Pu ) + ( Pu, Pu ) - ( u, u )\\
				&=& 2( u, u - Pu ) + ( Pu, Pu ) - ( u, u )\\
( u, u ) - ( Pu, Pu )  & = & 2( u, u - Pu ) - ||u - Pu||^{2}	
\end{eqnarray*}
Now since $u \in B_{m}$, $<u,u> = 1$. So we have
\begin{eqnarray*}
1 - ||Pu||^{2}  & = & 2( u, u - Pu ) - ||u - Pu||^{2}	
\end{eqnarray*}
Now the right hand side, $1 - ||Pu||^{2} \leq 1 - \mu_{m}^{h} = \sigma_{m}^{h}$ for all $u \in B_{m}$. So then we must have that
\begin{eqnarray*}
\sigma_{m}^{h} = \max\left\{ 2( u,u-Pu )-||u-Pu||^{2} \ | \ u \in B_{m} \right \}
\end{eqnarray*}\qed
\end{comment}

Next we introduce some new notation to ease the writing. For any $u \in E_{m}$, let $u^{*} = \sum_{i=1}^{m} c_{i}\lambda_{i}^{-1}u_{i}$ where $u = \sum_{i=1}^{m} c_{i}u_{i}$.\\

%\newpage

\textbf{Proposition 7.3:} For any $u \in E_{m}$
\begin{eqnarray*}
( u, u - Pu ) = b(u^{*} - Pu^{*}, u -Pu)
\end{eqnarray*}

\begin{comment}
\textbf{Proof:} For any $i = 1,2,...,m$, we have that
\begin{eqnarray*}
\lambda_{i}( u_{i},u-Pu ) &=&  b(u_{i}, u-Pu)\\
									&=& b(u_{i}, u-Pu) - b(u-Pu,Pu_{i}) \ \textrm{ (by definition of Rayleigh-Ritz Projection)}\\
									&=&  b(u_{i}, u-Pu) - b(Pu_{i},u-Pu)\\
									&=&  b(u_{i}-Pu_{i}, u-Pu)
\end{eqnarray*}
So multiplying by $c_{i}\lambda_{i}^{-1}$ and summation over i gives:
\begin{eqnarray*}
\sum_{i=1}^{m} c_{i}\lambda_{i}^{-1}\lambda_{i}( u_{i},u-Pu ) &=& \sum_{i=1}^{m} c_{i}\lambda_{i}^{-1}b(u_{i}-Pu_{i}, u-Pu)\\
									&=& b(\sum_{i=1}^{m} c_{i}\lambda_{i}^{-1}u_{i}-\sum_{i=1}^{m} c_{i}\lambda_{i}^{-1}Pu_{i}, u-Pu)\\
									&=& b(u^{*}-Pu^{*}, u-Pu)
\end{eqnarray*}
So $( u,u-Pu ) = b(u^{*}-Pu^{*}, u-Pu)$.\qed\\
\end{comment}

\textbf{Lemma 7.2:} $\sigma_{m}^{h} \leq \max \left\{2||u^{*}- Pu^{*}|| ||u-Pu|| \ | \ u \in B_{m} \right\}$\\

\begin{comment}
\textbf{Proof:} From Proposition 7.1, we have
\begin{eqnarray*}
\sigma_{m}^{h} &=& \max\left\{ 2( u,u-Pu )-||u-Pu||^{2} \ | \ u \in B_{m} \right \} \\
				&\leq & \max\left\{ 2( u,u-Pu ) \ | \ u \in B_{m} \right \}
\end{eqnarray*} since $||u-Pu||^{2} \geq 0$.\\
\end{comment}

Using Proposition 7.3
\begin{eqnarray*}
\sigma_{m}^{h} &\leq & \max\left\{ 2b(u^{*} - Pu^{*}, u -Pu) \ | \ u \in B_{m} \right \}			
\end{eqnarray*}
Now
\begin{eqnarray*}
b(u^{*} - Pu^{*}, u -Pu) & \leq & ||u^{*} - Pu^{*}||||u -Pu|| \ \textrm{ (Schwartz inequality)} \\
\end{eqnarray*}
Thus  $\sigma_{m}^{h} \leq \max \left\{2||u^{*}- Pu^{*}|| ||u-Pu|| \ | \ u \in B_{m} \right\}$. \qed

%\newpage

\textbf{Proposition 7.3:} For any $\epsilon > 0$, there exists a $\delta >0$ such that for $h<\delta$,
\begin{eqnarray*}
||u^{*} - Pu^{*}|| < \epsilon \ \textrm{ for each } u \in B_{m}\\
||u - Pu|| < \epsilon \ \textrm{ for each } u \in B_{m}
\end{eqnarray*}

\begin{comment}
\textbf{Proof:}
Suppose we have a set of basis functions $\Phi \subset S^{h}$ such that each element in $\Phi$ has order k, i.e. for each $v \in \Phi$, $v^{(k)} = 0$. Now from the definition of the Rayleigh-Ritz Projection, we have that $Pu$ is the closest element in $S^{h}$ to $u$. In particular, $||u-Pu||\leq ||u-\Pi u||$, where $\Pi u$ is the interpolant of $u$ into $S^{h}$.\\


Now from, we use the Approximation Theorem, so that we have
\begin{eqnarray*}
||u - \Pi u|| \leq Ch^{k}||u^{(k)}||
\end{eqnarray*}
So for any $\epsilon > 0$, we can find a $\delta$ such that if $h<\delta$ then
\begin{eqnarray*}
||u - Pu|| \leq ||u - \Pi u|| \leq \epsilon
\end{eqnarray*}
A similar argument for $||u^{*} - Pu^{*}||$ proves the theorem.
\qed
\\
\end{comment}

\textbf{Lemma 7.4:} For any $\epsilon >0$ there exists a $\delta > 0$ such that
\begin{eqnarray*}
\sigma_{m}^{h} < \epsilon \ \textrm{ if } \ h < \delta
\end{eqnarray*}

\begin{comment}
\textbf{Proof:} For any $\epsilon > 0$ there exists a $\delta > 0$ such that if $h<\delta$, then
\begin{eqnarray*}
||u-Pu|| < \epsilon \ \textrm{ for each } \ u \in B_{m}
\end{eqnarray*}
This, together with Lemma 7.2 and Proposition 7.4 gives the result.\qed\\
\end{comment}

\textbf{Lemma 7.5:} There exists a $\delta > 0$ such that for $h < \delta$
\begin{eqnarray*}
\lambda_{m}^{h} - \lambda_{m} \leq 2\lambda_{m}\sigma_{m}^{h}
\end{eqnarray*}

\begin{comment}
\textbf{Proof:}
Using Lemma 7.4, choose $\delta$ such that $\sigma_{m}^{h} < \frac{1}{2}$. Then by Lemma 7.1 gives that $\lambda_{m}^{h} < 2\lambda_{m}$. So we have that
$\lambda_{m}^{h} - \lambda_{m} \leq 2\lambda_{m}\sigma_{m}^{h}$.
\qed
\end{comment}

\subsection{Convergence of the eigenfunctions}
So from Lemma 7.4, it is that if $h \rightarrow 0$, we will get convergence in the eigenvalues. The next step is now to use this, to show convergence in the eigenvectors.\\


\textbf{Lemma 7.6:}
\begin{eqnarray*}
b(u_{m}-u_{m}^{h},u_{m}-u_{m}^{h}) &=& \lambda_{m}( u_{m}-u_{m}^{h},u_{m}-u_{m}^{h} ) + \lambda_{m}^{h} - \lambda_{m}
\end{eqnarray*}

\begin{comment}
\textbf{Proof:}
\begin{eqnarray*}
b(u_{m}-u_{m}^{h},u_{m}-u_{m}^{h}) &=& b(u_{m},u_{m}) - 2b(u_{m},u^{h}_{m}) + b(u^{h}_{m},u^{h}_{m}) \\
								&=& \lambda_{m} ( u_{m}, u_{m} ) - 2\lambda_{m} ( u_{m}, u^{h}_{m} ) + \lambda_{m}^{h}( u_{m}^{h},u_{m}^{h} )\\
								&=&  \lambda_{m} - 2\lambda_{m} ( u_{m}, u^{h}_{m} ) + \lambda_{m}^{h} \\
								&=& 2\lambda_{m} - 2\lambda_{m} ( u_{m}, u^{h}_{m} ) + \lambda_{m}^{h} - \lambda_{m}\\
								&=& \lambda_{m}( u_{m}-u_{m}^{h},u_{m}-u_{m}^{h}) + \lambda_{m}^{h} - \lambda_{m}
\end{eqnarray*}
\qed
\end{comment}

We have already shown that $\lambda_{m}^{h} \rightarrow \lambda_{m}$. So it remains only to show that $( u_{m}-u_{m}^{h},u_{m}-u_{m}^{h}) \rightarrow 0$ as $h \rightarrow 0$


Now we have the possibility that an eigenvalue, say $\lambda_{m}$ has multiplicity $r>1$. This implies that there are at least two eigenfunctions that correspond to this eigenvalue.\\

The Euler-Bernoulli beam however, only has a repeated eigenvalue if we consider the free-free boundary conditions. Then the beam would have a zero eigenvalue with multiplicity more than 1. Our model, however, does not have repeated eigenvalues. So we may assume that all the eigenvalues has multiplicity $r=1$.\\


\textbf{Lemma 7.7:} For all m and j
\begin{eqnarray*}
(\lambda_{j}^{h} - \lambda_{m}) ( Pu_{m}, u_{j}^{h}) = \lambda_{m} ( u_{m}-Pu_{m},u_{j}^{h} )
\end{eqnarray*}

\begin{comment}
\textbf{Proof:}
Since the term $\lambda_{m}( Pu,u^{h}_{j})$ appears on both sides of the equation, it is only required to show that
\begin{eqnarray*}
\lambda_{j}^{h}( Pu, u_{j}^{h} ) &=& \lambda_{m} ( u, u_{j}^{h} )
\end{eqnarray*}
Now
\begin{eqnarray*}
\lambda_{j}^{h}( Pu, u_{j}^{h} ) &=& b(Pu,u_{j}^{h})\\
\lambda_{m} ( u, u_{j}^{h} ) &=& b(u,u_{j}^{h})
\end{eqnarray*} Since $u$ and $u_{j}^{h}$ are both eigenfunctions.\\


Then equality follows from the definitions of the projection P.
\qed
\end{comment}

The set $\left\{u_{1}^{h},u_{2}^{h},...,u_{N}^{h}\right\}$ then forms an orthonormal basis for $S^{h}$. So we can then write a projection $Pu_{m}$ as:
\begin{eqnarray}
Pu_{m} &=& \sum_{j=1}^{N} ( P u_{m} ,u_{j}^{h}) u_{j}^{h} \label{CV1}
\end{eqnarray}


From Lemma 7.8, we see that $( P_{m},u_{j}^{h} )$ is small if $\lambda_{m}^{h}$ is not close to $\lambda_{j}$. So then (\ref{CV1}) tells us then that $Pu_{m}$ is close to $u_{m}^{h}$. This will allow us to estimate $Pu_{m} - u_{m}^{h}$.\\


Since we have convergence in our eigenvalues, it follows that $\exists \rho > 0$ and $\exists \delta > 0$ such that if $h<\delta$ we have that
\begin{eqnarray}
|\lambda_{m} - \lambda_{j}^{h}| &>& \rho \ \ \textrm{ for all } \ j = 1,2,...,N
\end{eqnarray}
Therefore
\begin{eqnarray}
\frac{\lambda_{m}}{|\lambda_{m} - \lambda_{j}^{h}|} &\leq & \rho \ \ \textrm{ for all } \ j = 1,2,...,N
\end{eqnarray}


\textbf{Lemma 7.8:}
\begin{eqnarray*}
||Pu - QPu||^{2} &\leq & {\rho}^{2} ||u_{m} - Pu_{m}||^{2}
\end{eqnarray*}

\begin{comment}
\textbf{Proof:}

Now using Lemma 7.7:
\begin{eqnarray*}
||Pu - \beta u_{m}^{h}||^{2} &=& \sum_{j\neq m} \left(\frac{\lambda_{m}}{|\lambda_{m} - \lambda_{j}^{h}|}\right)^{2} ( u_{m} - Pu_{m} ,u_{j}^{h})^{2}\\
				&\leq & \rho^{2} \sum_{j\neq m} ( u_{m} - Pu_{m} ,u_{j}^{h})^{2} \\
				&\leq & \rho^{2} \sum_{j=1}^{N} ( u_{m} - Pu_{m} ,u_{j}^{h})^{2} \\
				& = & \rho^{2} ||u - Pu||^{2}\\
\end{eqnarray*}
\qed
\end{comment}

\textbf{Lemma 7.9:}
\begin{eqnarray*}
||u_{m} - \beta u_{m}^{h}|| &\leq & \left(1+\rho\right)||u_{m}-Pu_{m}||
\end{eqnarray*}

\begin{comment}
\textbf{Proof:}
\begin{eqnarray*}
||u_{m} - \beta u_{m}^{h}|| & \leq & ||u_{m}-Pu_{m}|| + ||Pu_{m} - \beta u_{m}^{h}|| \\
			& \leq & \left(1+\rho\right)||u_{m}-Pu_{m}|| \ \ \textrm{ (by Lemma 7.8)}
\end{eqnarray*}
\qed
\end{comment}



So again using the Approximation Theorem, we have that $||u_{m} - \beta u_{m}^{h}||\leq Ch^{k}||u^{k}||$.\\


\textbf{Proposition 7.4:}
\begin{eqnarray*}
||u_{m} -  u_{m}^{h}|| &\leq & 2||u_{m}-\beta u^{h}_{m}||
\end{eqnarray*}

\begin{comment}
\textbf{Proof:}
\begin{eqnarray*}
||u_{m} - u_{m}^{h}|| &=& ||u_{m} - \beta u_{m}^{h} + \beta u_{m}^{h} - u_{m}^{h}|| \\
					& \leq & ||u_{m} - \beta u_{m}^{h}|| + ||\beta u_{m}^{h} - u_{m}^{h}|| \\
					& = & 2||u_{m} - \beta u_{m}^{h}||	
\end{eqnarray*}
\qed
\end{comment}

Therefore we have that $||u_{m} -  u_{m}^{h}|| \leq Ch^{k}||u^{k}||$. So for any $\epsilon >0$ , we can find a $\delta >0$ so that if $h < \delta$, we have that $||u_{m} -  u_{m}^{h}|| < \epsilon$.

Finally, for elastic beam models like the Euler-Bernoulli beam model, the norm $||.||$ is the energy norm defined as
\begin{eqnarray*}
||u|| &=& \left(\int u^{2}\right)^{\frac{1}{2}}
\end{eqnarray*}


So clearly then we have that $u_{m}^{h} \rightarrow u_{m}$ as $h \rightarrow 0$.

\section{Example}
What moet hier kom?\\

dalk timoshenko beam sal beter wees met plaat of 2d model.\\

Consider a variational problem for a cantilever two-dimensional beam, with a reference configuration $\Omega$. In section \ref{sssec:2D_Model:Problem2D1}, the derivation of this variational problem is referred to as Problem 2D-1V.

\subsubsection{Problem 2D-1V}\label{sssec:2D_Model:Problem2D1V}
Find a function $u$ such that for all $t>0$, $u \in T(\Omega)$ and 
\begin{align}
	\int_{\Omega} (\partial_t^2 u)\cdot \phi \ dA = -b(u,\phi) - \int_{\Omega} Q\cdot\phi \ dA \label{eq:2D_Model:Problem2D1VEq}
\end{align}
for all $\phi \in T(\Omega)$.\\

With the test function space defined as
\subsubsection{Test functions}
\begin{eqnarray*}
	T(\Omega) & = & \left\{ \phi \in C^1(\bar{\Omega}) \ | \ \phi = 0 \ \textrm{ on } \ \Gamma \right\}.
\end{eqnarray*}




\subsection{Weak variational form}
For all $f,g \in T(\Omega)$ and $h \in C(\Omega)$, define the bilinear forms
\begin{eqnarray*}
	c(f,g) & = & \iint_\Omega f \cdot g \ dA, \\
\end{eqnarray*}
and integral
\begin{eqnarray*}
	(h,g) & = & \iint_\Omega h \cdot g \ dA. \\
\end{eqnarray*}
Recall from section  that the bilinear form $b$ is defined as
\begin{eqnarray*}
	b(u,\phi) & = & \frac{1}{\gamma(1-\nu^2)}\iint_{\Omega} (\partial_1 u_1 \partial_1 \phi_1 + \partial_2 u_2 \partial _2 \phi_2 + \nu\partial_1 u_1 \partial_2\phi_2 + \nu \partial_2 u_2 \partial_1 \phi_1 ) \ dA \\
	& + & \frac{1}{2\gamma(1+\nu)}\iint_{\Omega} (\partial_1 u_2 \partial_1 \phi_2 + \partial_1 u_2 \partial_2 \phi_1 + \partial_2 u_1 \partial_1\phi_2 + \partial_2 u_1 \partial_2\phi_1) \ dA.
\end{eqnarray*}

Define the inertia space $V$ as the closure of $T(\Omega)$ in $H := H^1(0,1)\times H^1(0,1)$. Denote $X = L^2(0,1)\times L^2(0,1)$. The inertia space is $W  = X$ with norm $||\cdot||_W = \sqrt{c(\cdot,\cdot)}$.

\subsubsection{Problem 2D-1W}
Find a function $u$ such that for all $t>0$, $u(t) \in V$ and $u''(t) \in W$, satisfying the following equation
\begin{eqnarray}
	c(u''(t),v) + b(u(t),v) & = & (Q(t),v)_X \ \ \ \textrm{ for each } v \in V.
\end{eqnarray}

\end{document}
